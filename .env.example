# LLM_BASE_URL="http://host.docker.internal" # for Ollama outside .devcontainer 
LLM_BASE_URL="http://localhost" # for .devcontainer Ollama
LLM_BASE_URL_PORT="11434"
TEXT_MODEL="qwen2:7b-instruct"
OPENAI_API_KEY="sk-proj-ollama-key" # if not using OpenAI this can be random